{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e114f281",
   "metadata": {},
   "source": [
    "# Model Stress Test ‚Äî Systematic Failure Analysis\n",
    "\n",
    "**Role**: Senior Quantitative Researcher (Model Validation & Stress Testing)\n",
    "\n",
    "## Objective\n",
    "\n",
    "This notebook **does not improve the model**. It systematically **breaks** it to understand:\n",
    "- Where does it fail?\n",
    "- Why does it fail?\n",
    "- Is the signal real or statistical coincidence?\n",
    "\n",
    "## Hard Constraints\n",
    "\n",
    "| ‚ùå FORBIDDEN | ‚úÖ ALLOWED |\n",
    "|-------------|------------|\n",
    "| Retraining | Diagnostics only |\n",
    "| Feature modification | Slicing existing predictions |\n",
    "| Backtests | IC/correlation analysis |\n",
    "| Strategy logic | Statistical tests |\n",
    "| Sharpe optimization | Failure interpretation |\n",
    "\n",
    "## Model Breakers Implemented\n",
    "\n",
    "1. **Time-Based Stress** ‚Äî Does the model rely on a specific regime?\n",
    "2. **Feature Ablation** ‚Äî Is it dependent on a narrow feature subset?\n",
    "3. **Permutation Tests** ‚Äî Is performance real or luck?\n",
    "4. **Noise Injection** ‚Äî Is it robust to small perturbations?\n",
    "5. **Horizon Mismatch** ‚Äî Is the signal horizon-specific?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f2d50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 1: SETUP & LOAD MODEL ARTIFACTS\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Paths\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "DATA_DIR = PROJECT_ROOT / 'data' / 'processed'\n",
    "TARGET_DIR = DATA_DIR / 'targets'\n",
    "MODEL_DIR = PROJECT_ROOT / 'outputs' / 'models'\n",
    "OUTPUT_DIR = PROJECT_ROOT / 'outputs' / 'stress_test'\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üî¨ MODEL STRESS TEST ‚Äî FAILURE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìÅ Project root: {PROJECT_ROOT}\")\n",
    "print(f\"üìÅ Output dir: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371504f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: LOAD DATA & MODEL\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüìÇ Loading model artifacts...\")\n",
    "\n",
    "# Load features (long format)\n",
    "features_df = pd.read_parquet(DATA_DIR / 'features_aligned_is.parquet')\n",
    "print(f\"   Features: {features_df.shape}\")\n",
    "\n",
    "# Load target (wide format -> convert to long)\n",
    "target_wide = pd.read_parquet(TARGET_DIR / 'primary_target_is.parquet')\n",
    "target_long = target_wide.stack().reset_index()\n",
    "target_long.columns = ['date', 'ticker', 'target']\n",
    "print(f\"   Target: {target_long.shape}\")\n",
    "\n",
    "# Load feature names\n",
    "with open(DATA_DIR / 'feature_names.txt', 'r') as f:\n",
    "    feature_names = [line.strip() for line in f.readlines()]\n",
    "print(f\"   Feature names: {len(feature_names)}\")\n",
    "\n",
    "# Load trained model\n",
    "model_path = MODEL_DIR / 'final_lgb_model.joblib'\n",
    "if model_path.exists():\n",
    "    model = joblib.load(model_path)\n",
    "    print(f\"   ‚úÖ Loaded model: {model_path.name}\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è No model found at {model_path}\")\n",
    "    print(f\"   Will generate synthetic predictions for demonstration\")\n",
    "    model = None\n",
    "\n",
    "# Load target metadata\n",
    "with open(TARGET_DIR / 'target_metadata.json', 'r') as f:\n",
    "    target_config = json.load(f)\n",
    "print(f\"   Target horizon: {target_config['horizon']} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d009e448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 3: GENERATE PREDICTIONS (if not already saved)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüîÆ Generating predictions...\")\n",
    "\n",
    "# Merge features with target\n",
    "panel = features_df.merge(target_long, on=['date', 'ticker'], how='inner')\n",
    "print(f\"   Merged panel: {panel.shape}\")\n",
    "\n",
    "# Drop rows with missing target\n",
    "panel = panel.dropna(subset=['target'])\n",
    "print(f\"   After dropping NaN targets: {panel.shape}\")\n",
    "\n",
    "# Generate predictions\n",
    "X = panel[feature_names].values\n",
    "y = panel['target'].values\n",
    "\n",
    "if model is not None:\n",
    "    predictions = model.predict(X)\n",
    "else:\n",
    "    # Generate weak synthetic signal for demonstration\n",
    "    # This mimics a model with IC ~ 0.03\n",
    "    noise = np.random.randn(len(y))\n",
    "    predictions = 0.03 * y + 0.97 * noise\n",
    "    print(\"   ‚ö†Ô∏è Using synthetic predictions (model not found)\")\n",
    "\n",
    "panel['prediction'] = predictions\n",
    "\n",
    "# Compute baseline IC\n",
    "baseline_ic = np.corrcoef(predictions, y)[0, 1]\n",
    "print(f\"\\nüìä Baseline IC: {baseline_ic:.4f}\")\n",
    "print(f\"   (This is what we're trying to break)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daafec9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model Breaker 1: Time-Based Stress (Regime Dependence)\n",
    "\n",
    "### Hypothesis Being Tested\n",
    "\n",
    "> **Does the model rely on a specific market regime?**\n",
    "\n",
    "If the model was trained predominantly on bull markets, it may fail in bear markets.\n",
    "If it learned volatility patterns from 2020, it may not generalize.\n",
    "\n",
    "### Expected Failure If Model Is Fragile\n",
    "\n",
    "- IC flips sign in certain regimes\n",
    "- Prediction bias changes across regimes\n",
    "- Performance clusters in specific time periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8283751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BREAKER 1: TIME-BASED STRESS ‚Äî REGIME SLICING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üî® BREAKER 1: TIME-BASED STRESS (Regime Dependence)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Step 1: Compute daily cross-sectional IC\n",
    "def compute_daily_ic(df):\n",
    "    \"\"\"Compute IC for each date.\"\"\"\n",
    "    ic_by_date = df.groupby('date').apply(\n",
    "        lambda x: x['prediction'].corr(x['target']) if len(x) > 5 else np.nan\n",
    "    )\n",
    "    return ic_by_date\n",
    "\n",
    "daily_ic = compute_daily_ic(panel)\n",
    "daily_ic = daily_ic.dropna()\n",
    "\n",
    "print(f\"\\nüìä Daily IC Statistics:\")\n",
    "print(f\"   Mean IC: {daily_ic.mean():.4f}\")\n",
    "print(f\"   Std IC: {daily_ic.std():.4f}\")\n",
    "print(f\"   IC > 0: {(daily_ic > 0).mean()*100:.1f}%\")\n",
    "\n",
    "# Step 2: Define regimes\n",
    "# Use rolling volatility of the cross-sectional mean return as regime indicator\n",
    "daily_mean_target = panel.groupby('date')['target'].mean()\n",
    "rolling_vol = daily_mean_target.rolling(21).std()\n",
    "\n",
    "# High vol vs low vol regimes (median split)\n",
    "vol_median = rolling_vol.median()\n",
    "high_vol_dates = rolling_vol[rolling_vol > vol_median].index\n",
    "low_vol_dates = rolling_vol[rolling_vol <= vol_median].index\n",
    "\n",
    "# Bull vs bear regimes (based on cumulative return)\n",
    "cumret = daily_mean_target.cumsum()\n",
    "rolling_trend = cumret.diff(63)  # 3-month trend\n",
    "bull_dates = rolling_trend[rolling_trend > 0].index\n",
    "bear_dates = rolling_trend[rolling_trend <= 0].index\n",
    "\n",
    "print(f\"\\nüìÖ Regime Splits:\")\n",
    "print(f\"   High Vol dates: {len(high_vol_dates)}\")\n",
    "print(f\"   Low Vol dates: {len(low_vol_dates)}\")\n",
    "print(f\"   Bull dates: {len(bull_dates)}\")\n",
    "print(f\"   Bear dates: {len(bear_dates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e618e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BREAKER 1 (cont): REGIME IC ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "# Compute IC per regime\n",
    "regimes = {\n",
    "    'High Volatility': high_vol_dates,\n",
    "    'Low Volatility': low_vol_dates,\n",
    "    'Bull Market': bull_dates,\n",
    "    'Bear Market': bear_dates\n",
    "}\n",
    "\n",
    "regime_stats = []\n",
    "for regime_name, regime_dates in regimes.items():\n",
    "    regime_ic = daily_ic[daily_ic.index.isin(regime_dates)]\n",
    "    if len(regime_ic) > 10:\n",
    "        regime_stats.append({\n",
    "            'Regime': regime_name,\n",
    "            'N_Days': len(regime_ic),\n",
    "            'Mean_IC': regime_ic.mean(),\n",
    "            'Std_IC': regime_ic.std(),\n",
    "            'IC_t_stat': regime_ic.mean() / (regime_ic.std() / np.sqrt(len(regime_ic))),\n",
    "            'IC_positive_pct': (regime_ic > 0).mean() * 100\n",
    "        })\n",
    "\n",
    "regime_df = pd.DataFrame(regime_stats)\n",
    "print(\"\\nüìä IC by Regime:\")\n",
    "print(regime_df.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# IC time series with regime shading\n",
    "ax = axes[0]\n",
    "ax.plot(daily_ic.index, daily_ic.values, alpha=0.5, linewidth=0.5, color='gray')\n",
    "rolling_ic = daily_ic.rolling(21).mean()\n",
    "ax.plot(rolling_ic.index, rolling_ic.values, color='blue', linewidth=1.5, label='21d Rolling IC')\n",
    "ax.axhline(0, color='red', linestyle='--', linewidth=1)\n",
    "ax.axhline(baseline_ic, color='green', linestyle=':', label=f'Baseline IC: {baseline_ic:.4f}')\n",
    "ax.set_title('Daily IC Over Time', fontweight='bold')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('IC')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# IC boxplot by regime\n",
    "ax = axes[1]\n",
    "regime_ic_data = []\n",
    "regime_labels = []\n",
    "for regime_name, regime_dates in regimes.items():\n",
    "    regime_ic = daily_ic[daily_ic.index.isin(regime_dates)].dropna()\n",
    "    regime_ic_data.append(regime_ic.values)\n",
    "    regime_labels.append(regime_name.replace(' ', '\\n'))\n",
    "\n",
    "bp = ax.boxplot(regime_ic_data, labels=regime_labels, patch_artist=True)\n",
    "colors = ['coral', 'lightblue', 'lightgreen', 'salmon']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "ax.axhline(0, color='red', linestyle='--', linewidth=1)\n",
    "ax.set_title('IC Distribution by Regime', fontweight='bold')\n",
    "ax.set_ylabel('IC')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'breaker1_regime_ic.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Interpretation\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìã BREAKER 1 INTERPRETATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ic_range = regime_df['Mean_IC'].max() - regime_df['Mean_IC'].min()\n",
    "sign_flip = (regime_df['Mean_IC'] < 0).any()\n",
    "\n",
    "if sign_flip:\n",
    "    verdict = \"‚ùå FAIL\"\n",
    "    interpretation = \"IC flips sign across regimes ‚Äî model is regime-dependent\"\n",
    "elif ic_range > 0.02:\n",
    "    verdict = \"‚ö†Ô∏è WEAK\"\n",
    "    interpretation = f\"IC varies by {ic_range:.4f} across regimes ‚Äî moderate regime sensitivity\"\n",
    "else:\n",
    "    verdict = \"‚úÖ PASS\"\n",
    "    interpretation = \"IC is stable across regimes\"\n",
    "\n",
    "print(f\"\\n   Verdict: {verdict}\")\n",
    "print(f\"   {interpretation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0de3c5a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model Breaker 2: Feature Ablation (Structural Dependency)\n",
    "\n",
    "### Hypothesis Being Tested\n",
    "\n",
    "> **Is the model dependent on a narrow subset of features?**\n",
    "\n",
    "A robust model should degrade gracefully when feature families are removed.\n",
    "An overfit model will collapse when its \"crutch\" features are ablated.\n",
    "\n",
    "### Expected Failure If Model Is Fragile\n",
    "\n",
    "- Removing one feature family destroys all signal\n",
    "- Model relies entirely on momentum OR volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4173b351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BREAKER 2: FEATURE ABLATION ‚Äî STRUCTURAL DEPENDENCY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üî® BREAKER 2: FEATURE ABLATION (Structural Dependency)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Define feature families\n",
    "feature_families = {\n",
    "    'momentum': [f for f in feature_names if 'mom' in f.lower()],\n",
    "    'volatility': [f for f in feature_names if 'vol' in f.lower()],\n",
    "    'kalman': [f for f in feature_names if 'kalman' in f.lower()],\n",
    "    'regime': [f for f in feature_names if 'regime' in f.lower()],\n",
    "    'cross_sectional': [f for f in feature_names if 'cs_' in f.lower()],\n",
    "    'technical': [f for f in feature_names if any(x in f.lower() for x in ['ma_', 'bb_', 'rsi'])]\n",
    "}\n",
    "\n",
    "print(\"\\nüìä Feature Families:\")\n",
    "for family, features in feature_families.items():\n",
    "    print(f\"   {family}: {len(features)} features\")\n",
    "    if features:\n",
    "        print(f\"      Examples: {features[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58733138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BREAKER 2 (cont): ABLATION ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "# Function to compute IC with ablated features\n",
    "def compute_ablated_ic(panel, feature_names, ablate_features, model):\n",
    "    \"\"\"\n",
    "    Zero out ablated features and recompute predictions.\n",
    "    NOTE: We don't retrain ‚Äî we mask inputs to the existing model.\n",
    "    \"\"\"\n",
    "    X = panel[feature_names].copy()\n",
    "    X[ablate_features] = 0  # Zero out ablated features\n",
    "    \n",
    "    if model is not None:\n",
    "        preds = model.predict(X.values)\n",
    "    else:\n",
    "        # For synthetic model, simulate ablation effect\n",
    "        # Assume each family contributes ~equally to signal\n",
    "        ablate_frac = len(ablate_features) / len(feature_names)\n",
    "        signal_remaining = 1 - ablate_frac * 0.8  # 80% signal from features\n",
    "        noise = np.random.randn(len(panel))\n",
    "        preds = signal_remaining * 0.03 * panel['target'].values + noise\n",
    "    \n",
    "    ic = np.corrcoef(preds, panel['target'].values)[0, 1]\n",
    "    return ic\n",
    "\n",
    "# Run ablation for each family\n",
    "ablation_results = [{'Family': 'None (Baseline)', 'IC': baseline_ic, 'IC_Delta': 0, 'Pct_Retained': 100}]\n",
    "\n",
    "for family, features in feature_families.items():\n",
    "    if features:  # Only ablate if family has features\n",
    "        ablated_ic = compute_ablated_ic(panel, feature_names, features, model)\n",
    "        ic_delta = ablated_ic - baseline_ic\n",
    "        pct_retained = (ablated_ic / baseline_ic) * 100 if baseline_ic != 0 else 0\n",
    "        \n",
    "        ablation_results.append({\n",
    "            'Family': family,\n",
    "            'IC': ablated_ic,\n",
    "            'IC_Delta': ic_delta,\n",
    "            'Pct_Retained': pct_retained\n",
    "        })\n",
    "\n",
    "ablation_df = pd.DataFrame(ablation_results)\n",
    "print(\"\\nüìä Ablation Results:\")\n",
    "print(ablation_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c714279c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BREAKER 2 (cont): VISUALIZATION & INTERPRETATION\n",
    "# =============================================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "families = ablation_df['Family'].tolist()\n",
    "ics = ablation_df['IC'].tolist()\n",
    "colors = ['green' if f == 'None (Baseline)' else 'steelblue' for f in families]\n",
    "\n",
    "bars = ax.barh(families, ics, color=colors, edgecolor='black')\n",
    "ax.axvline(baseline_ic, color='green', linestyle='--', linewidth=2, label=f'Baseline: {baseline_ic:.4f}')\n",
    "ax.axvline(0, color='red', linestyle='-', linewidth=1)\n",
    "\n",
    "# Add value labels\n",
    "for bar, ic in zip(bars, ics):\n",
    "    ax.annotate(f'{ic:.4f}', xy=(ic, bar.get_y() + bar.get_height()/2),\n",
    "               ha='left' if ic >= 0 else 'right', va='center', fontsize=10)\n",
    "\n",
    "ax.set_xlabel('IC After Ablation')\n",
    "ax.set_title('Feature Family Ablation: IC Impact\\n(Lower = More Dependent)', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'breaker2_feature_ablation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Interpretation\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìã BREAKER 2 INTERPRETATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find most critical family\n",
    "ablation_no_baseline = ablation_df[ablation_df['Family'] != 'None (Baseline)']\n",
    "most_critical = ablation_no_baseline.loc[ablation_no_baseline['IC'].idxmin()]\n",
    "max_drop = baseline_ic - most_critical['IC']\n",
    "\n",
    "if most_critical['IC'] <= 0:\n",
    "    verdict = \"‚ùå FAIL\"\n",
    "    interpretation = f\"Removing {most_critical['Family']} destroys signal (IC goes to {most_critical['IC']:.4f})\"\n",
    "elif most_critical['Pct_Retained'] < 50:\n",
    "    verdict = \"‚ö†Ô∏è WEAK\"\n",
    "    interpretation = f\"Model heavily reliant on {most_critical['Family']} (retains only {most_critical['Pct_Retained']:.1f}% IC)\"\n",
    "else:\n",
    "    verdict = \"‚úÖ PASS\"\n",
    "    interpretation = \"No single feature family dominates ‚Äî model has diverse signal sources\"\n",
    "\n",
    "print(f\"\\n   Most critical family: {most_critical['Family']}\")\n",
    "print(f\"   Verdict: {verdict}\")\n",
    "print(f\"   {interpretation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7271af38",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model Breaker 3: Permutation Tests (Luck Detection)\n",
    "\n",
    "### Hypothesis Being Tested\n",
    "\n",
    "> **Does the model exploit real structure or random correlations?**\n",
    "\n",
    "If performance is indistinguishable from shuffled data, the model learned noise.\n",
    "\n",
    "### Expected Failure If Model Is Fragile\n",
    "\n",
    "- Real IC lies inside the null distribution\n",
    "- p-value > 0.05 (cannot reject null hypothesis of no signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028de24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BREAKER 3: PERMUTATION TESTS ‚Äî LUCK DETECTION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üî® BREAKER 3: PERMUTATION TESTS (Luck Detection)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "N_PERMUTATIONS = 200\n",
    "\n",
    "# Test 1: Label Permutation (shuffle targets globally)\n",
    "print(\"\\nüé≤ Test 1: Label Permutation (shuffle targets)...\")\n",
    "label_perm_ics = []\n",
    "for i in range(N_PERMUTATIONS):\n",
    "    shuffled_target = np.random.permutation(panel['target'].values)\n",
    "    ic = np.corrcoef(panel['prediction'].values, shuffled_target)[0, 1]\n",
    "    label_perm_ics.append(ic)\n",
    "label_perm_ics = np.array(label_perm_ics)\n",
    "\n",
    "# Test 2: Time Permutation (shuffle predictions over time)\n",
    "print(\"üé≤ Test 2: Time Permutation (shuffle predictions over time)...\")\n",
    "time_perm_ics = []\n",
    "unique_dates = panel['date'].unique()\n",
    "for i in range(N_PERMUTATIONS):\n",
    "    # Shuffle which date's predictions go where\n",
    "    shuffled_dates = np.random.permutation(unique_dates)\n",
    "    date_map = dict(zip(unique_dates, shuffled_dates))\n",
    "    panel_shuffled = panel.copy()\n",
    "    panel_shuffled['shuffled_date'] = panel_shuffled['date'].map(date_map)\n",
    "    panel_shuffled = panel_shuffled.sort_values(['shuffled_date', 'ticker'])\n",
    "    ic = np.corrcoef(panel_shuffled['prediction'].values, panel['target'].values)[0, 1]\n",
    "    time_perm_ics.append(ic)\n",
    "time_perm_ics = np.array(time_perm_ics)\n",
    "\n",
    "# Test 3: Cross-sectional Permutation (shuffle predictions across assets at fixed t)\n",
    "print(\"üé≤ Test 3: Cross-sectional Permutation (shuffle across assets)...\")\n",
    "cs_perm_ics = []\n",
    "for i in range(N_PERMUTATIONS):\n",
    "    panel_shuffled = panel.copy()\n",
    "    # Shuffle predictions within each date\n",
    "    panel_shuffled['prediction'] = panel_shuffled.groupby('date')['prediction'].transform(\n",
    "        lambda x: np.random.permutation(x.values)\n",
    "    )\n",
    "    ic = np.corrcoef(panel_shuffled['prediction'].values, panel_shuffled['target'].values)[0, 1]\n",
    "    cs_perm_ics.append(ic)\n",
    "cs_perm_ics = np.array(cs_perm_ics)\n",
    "\n",
    "print(\"   Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab6a053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BREAKER 3 (cont): PERMUTATION VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "permutation_tests = [\n",
    "    ('Label Permutation', label_perm_ics),\n",
    "    ('Time Permutation', time_perm_ics),\n",
    "    ('Cross-Sectional Permutation', cs_perm_ics)\n",
    "]\n",
    "\n",
    "perm_results = []\n",
    "\n",
    "for ax, (test_name, null_dist) in zip(axes, permutation_tests):\n",
    "    # Plot null distribution\n",
    "    ax.hist(null_dist, bins=30, alpha=0.7, color='gray', edgecolor='black', label='Null Distribution')\n",
    "    \n",
    "    # Plot real IC\n",
    "    ax.axvline(baseline_ic, color='red', linewidth=2, linestyle='-', label=f'Real IC: {baseline_ic:.4f}')\n",
    "    \n",
    "    # Compute p-value (two-tailed)\n",
    "    p_value = np.mean(np.abs(null_dist) >= np.abs(baseline_ic))\n",
    "    \n",
    "    # 95% confidence interval\n",
    "    ci_low, ci_high = np.percentile(null_dist, [2.5, 97.5])\n",
    "    ax.axvline(ci_low, color='orange', linestyle='--', label=f'95% CI')\n",
    "    ax.axvline(ci_high, color='orange', linestyle='--')\n",
    "    \n",
    "    ax.set_title(f'{test_name}\\np-value: {p_value:.4f}', fontweight='bold')\n",
    "    ax.set_xlabel('IC')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    perm_results.append({\n",
    "        'Test': test_name,\n",
    "        'Real_IC': baseline_ic,\n",
    "        'Null_Mean': null_dist.mean(),\n",
    "        'Null_Std': null_dist.std(),\n",
    "        'p_value': p_value,\n",
    "        'Significant': p_value < 0.05\n",
    "    })\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'breaker3_permutation_tests.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "perm_df = pd.DataFrame(perm_results)\n",
    "print(\"\\nüìä Permutation Test Results:\")\n",
    "print(perm_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eceaf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BREAKER 3 (cont): INTERPRETATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìã BREAKER 3 INTERPRETATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "all_significant = perm_df['Significant'].all()\n",
    "any_significant = perm_df['Significant'].any()\n",
    "min_pvalue = perm_df['p_value'].min()\n",
    "\n",
    "if all_significant:\n",
    "    verdict = \"‚úÖ PASS\"\n",
    "    interpretation = f\"All permutation tests significant (min p={min_pvalue:.4f}) ‚Äî signal is real\"\n",
    "elif any_significant:\n",
    "    failed_tests = perm_df[~perm_df['Significant']]['Test'].tolist()\n",
    "    verdict = \"‚ö†Ô∏è WEAK\"\n",
    "    interpretation = f\"Failed: {', '.join(failed_tests)} ‚Äî partial signal, possible overfitting\"\n",
    "else:\n",
    "    verdict = \"‚ùå FAIL\"\n",
    "    interpretation = \"No test significant ‚Äî performance is indistinguishable from luck\"\n",
    "\n",
    "print(f\"\\n   Verdict: {verdict}\")\n",
    "print(f\"   {interpretation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d31f40",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model Breaker 4: Noise Injection (Robustness)\n",
    "\n",
    "### Hypothesis Being Tested\n",
    "\n",
    "> **Is the model sensitive to small perturbations in inputs?**\n",
    "\n",
    "A robust model should be stable under small input noise.\n",
    "An overfit model will collapse because it memorized exact feature values.\n",
    "\n",
    "### Expected Failure If Model Is Fragile\n",
    "\n",
    "- IC collapses under 1-5% noise\n",
    "- Prediction correlation drops sharply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03435b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BREAKER 4: NOISE INJECTION ‚Äî ROBUSTNESS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üî® BREAKER 4: NOISE INJECTION (Robustness)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "noise_levels = [0.0, 0.01, 0.02, 0.05, 0.10, 0.20]\n",
    "noise_results = []\n",
    "\n",
    "X_original = panel[feature_names].values\n",
    "y = panel['target'].values\n",
    "original_preds = panel['prediction'].values\n",
    "\n",
    "for alpha in noise_levels:\n",
    "    print(f\"   Testing noise level Œ± = {alpha:.2f}...\")\n",
    "    \n",
    "    # Add Gaussian noise: X' = X + Œµ, Œµ ~ N(0, Œ± * œÉ_X)\n",
    "    if alpha == 0:\n",
    "        X_noisy = X_original\n",
    "    else:\n",
    "        feature_stds = np.std(X_original, axis=0)\n",
    "        noise = np.random.randn(*X_original.shape) * alpha * feature_stds\n",
    "        X_noisy = X_original + noise\n",
    "    \n",
    "    # Get predictions on noisy features\n",
    "    if model is not None:\n",
    "        noisy_preds = model.predict(X_noisy)\n",
    "    else:\n",
    "        # Simulate noise effect\n",
    "        signal_decay = np.exp(-alpha * 5)  # Exponential decay\n",
    "        noisy_preds = signal_decay * original_preds + (1 - signal_decay) * np.random.randn(len(y)) * np.std(original_preds)\n",
    "    \n",
    "    # Compute metrics\n",
    "    ic_noisy = np.corrcoef(noisy_preds, y)[0, 1]\n",
    "    pred_corr = np.corrcoef(noisy_preds, original_preds)[0, 1]\n",
    "    \n",
    "    noise_results.append({\n",
    "        'Noise_Level': alpha,\n",
    "        'IC': ic_noisy,\n",
    "        'IC_Retained_Pct': (ic_noisy / baseline_ic) * 100 if baseline_ic != 0 else 0,\n",
    "        'Pred_Correlation': pred_corr\n",
    "    })\n",
    "\n",
    "noise_df = pd.DataFrame(noise_results)\n",
    "print(\"\\nüìä Noise Injection Results:\")\n",
    "print(noise_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5d0e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BREAKER 4 (cont): VISUALIZATION & INTERPRETATION\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# IC vs noise level\n",
    "ax = axes[0]\n",
    "ax.plot(noise_df['Noise_Level'], noise_df['IC'], 'o-', color='blue', linewidth=2, markersize=8)\n",
    "ax.axhline(baseline_ic, color='green', linestyle='--', label=f'Baseline: {baseline_ic:.4f}')\n",
    "ax.axhline(0, color='red', linestyle='-', linewidth=1)\n",
    "ax.fill_between(noise_df['Noise_Level'], noise_df['IC'], baseline_ic, alpha=0.3, color='red')\n",
    "ax.set_xlabel('Noise Level (Œ±)')\n",
    "ax.set_ylabel('IC')\n",
    "ax.set_title('IC Decay Under Noise Injection', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Prediction stability\n",
    "ax = axes[1]\n",
    "ax.plot(noise_df['Noise_Level'], noise_df['Pred_Correlation'], 'o-', color='purple', linewidth=2, markersize=8)\n",
    "ax.axhline(1.0, color='green', linestyle='--', label='Perfect stability')\n",
    "ax.axhline(0.9, color='orange', linestyle=':', label='90% threshold')\n",
    "ax.set_xlabel('Noise Level (Œ±)')\n",
    "ax.set_ylabel('Prediction Correlation')\n",
    "ax.set_title('Prediction Stability Under Noise', fontweight='bold')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'breaker4_noise_injection.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Interpretation\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìã BREAKER 4 INTERPRETATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check IC at 5% noise\n",
    "ic_at_5pct = noise_df[noise_df['Noise_Level'] == 0.05]['IC'].values[0]\n",
    "ic_retained_at_5pct = noise_df[noise_df['Noise_Level'] == 0.05]['IC_Retained_Pct'].values[0]\n",
    "\n",
    "if ic_at_5pct <= 0:\n",
    "    verdict = \"‚ùå FAIL\"\n",
    "    interpretation = f\"IC collapses to {ic_at_5pct:.4f} under 5% noise ‚Äî severe overfitting\"\n",
    "elif ic_retained_at_5pct < 50:\n",
    "    verdict = \"‚ö†Ô∏è WEAK\"\n",
    "    interpretation = f\"Only {ic_retained_at_5pct:.1f}% IC retained at 5% noise ‚Äî moderate overfitting\"\n",
    "else:\n",
    "    verdict = \"‚úÖ PASS\"\n",
    "    interpretation = f\"{ic_retained_at_5pct:.1f}% IC retained at 5% noise ‚Äî model is robust\"\n",
    "\n",
    "print(f\"\\n   IC at 5% noise: {ic_at_5pct:.4f}\")\n",
    "print(f\"   Verdict: {verdict}\")\n",
    "print(f\"   {interpretation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357b430f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model Breaker 5: Horizon Mismatch (Temporal Validity)\n",
    "\n",
    "### Hypothesis Being Tested\n",
    "\n",
    "> **Is the model learning a true medium-horizon signal or a fragile timing artifact?**\n",
    "\n",
    "If the model only predicts 5-day returns and fails at all other horizons,\n",
    "it may have learned horizon-specific noise rather than fundamental alpha.\n",
    "\n",
    "### Expected Failure If Model Is Fragile\n",
    "\n",
    "- IC exists only at the training horizon (5 days)\n",
    "- IC flips sign at other horizons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df6c73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BREAKER 5: HORIZON MISMATCH ‚Äî TEMPORAL VALIDITY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üî® BREAKER 5: HORIZON MISMATCH (Temporal Validity)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load returns to compute different horizons\n",
    "returns_is = pd.read_parquet(DATA_DIR.parent / 'raw' / 'returns_is.parquet') if (DATA_DIR.parent / 'raw' / 'returns_is.parquet').exists() else None\n",
    "\n",
    "if returns_is is None:\n",
    "    # Reconstruct returns from target (which is volnorm, so we need raw returns)\n",
    "    raw_target = pd.read_parquet(TARGET_DIR / 'raw_return_is.parquet')\n",
    "    print(\"   Using raw_return target for horizon analysis\")\n",
    "else:\n",
    "    raw_target = None\n",
    "\n",
    "# Test horizons\n",
    "horizons = [1, 3, 5, 10, 21]\n",
    "horizon_results = []\n",
    "\n",
    "print(\"\\n   Computing IC at different horizons...\")\n",
    "\n",
    "for h in horizons:\n",
    "    print(f\"   Horizon = {h} days...\")\n",
    "    \n",
    "    if raw_target is not None:\n",
    "        # Compute h-day forward return from raw return target (which is 5-day)\n",
    "        # This is approximate - we scale by horizon ratio\n",
    "        scale_factor = h / 5.0\n",
    "        target_h = raw_target * scale_factor  # Crude approximation\n",
    "    else:\n",
    "        # Use the existing target with horizon scaling\n",
    "        target_h = target_wide * (h / 5.0)\n",
    "    \n",
    "    # Flatten and align with predictions\n",
    "    target_h_long = target_h.stack().reset_index()\n",
    "    target_h_long.columns = ['date', 'ticker', f'target_{h}d']\n",
    "    \n",
    "    # Merge with panel\n",
    "    panel_h = panel[['date', 'ticker', 'prediction']].merge(target_h_long, on=['date', 'ticker'], how='inner')\n",
    "    panel_h = panel_h.dropna()\n",
    "    \n",
    "    if len(panel_h) > 100:\n",
    "        ic_h = np.corrcoef(panel_h['prediction'], panel_h[f'target_{h}d'])[0, 1]\n",
    "    else:\n",
    "        ic_h = np.nan\n",
    "    \n",
    "    horizon_results.append({\n",
    "        'Horizon': h,\n",
    "        'IC': ic_h,\n",
    "        'IC_vs_5d': ic_h / baseline_ic if baseline_ic != 0 else np.nan\n",
    "    })\n",
    "\n",
    "horizon_df = pd.DataFrame(horizon_results)\n",
    "print(\"\\nüìä Horizon Analysis Results:\")\n",
    "print(horizon_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b01727e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BREAKER 5 (cont): VISUALIZATION & INTERPRETATION\n",
    "# =============================================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.plot(horizon_df['Horizon'], horizon_df['IC'], 'o-', color='blue', linewidth=2, markersize=10)\n",
    "ax.axhline(0, color='red', linestyle='-', linewidth=1)\n",
    "ax.axvline(5, color='green', linestyle='--', linewidth=2, label='Training Horizon (5d)')\n",
    "\n",
    "# Highlight the training horizon\n",
    "training_ic = horizon_df[horizon_df['Horizon'] == 5]['IC'].values[0]\n",
    "ax.scatter([5], [training_ic], s=200, color='green', zorder=5, marker='*')\n",
    "\n",
    "ax.set_xlabel('Horizon (days)')\n",
    "ax.set_ylabel('IC')\n",
    "ax.set_title('IC vs Prediction Horizon\\n(Does signal exist at multiple horizons?)', fontweight='bold')\n",
    "ax.set_xticks(horizons)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add IC values as labels\n",
    "for _, row in horizon_df.iterrows():\n",
    "    ax.annotate(f'{row[\"IC\"]:.4f}', xy=(row['Horizon'], row['IC']),\n",
    "               xytext=(5, 10), textcoords='offset points', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'breaker5_horizon_mismatch.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Interpretation\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìã BREAKER 5 INTERPRETATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check if IC exists at multiple horizons\n",
    "positive_horizons = horizon_df[horizon_df['IC'] > 0]['Horizon'].tolist()\n",
    "ic_at_1d = horizon_df[horizon_df['Horizon'] == 1]['IC'].values[0]\n",
    "ic_at_21d = horizon_df[horizon_df['Horizon'] == 21]['IC'].values[0]\n",
    "\n",
    "if len(positive_horizons) == 1:\n",
    "    verdict = \"‚ùå FAIL\"\n",
    "    interpretation = f\"IC only positive at horizon {positive_horizons[0]}d ‚Äî horizon overfitting\"\n",
    "elif ic_at_1d < 0 or ic_at_21d < 0:\n",
    "    verdict = \"‚ö†Ô∏è WEAK\"\n",
    "    interpretation = \"IC flips sign at some horizons ‚Äî signal may be timing-dependent\"\n",
    "else:\n",
    "    verdict = \"‚úÖ PASS\"\n",
    "    interpretation = f\"IC positive at horizons {positive_horizons} ‚Äî signal generalizes across horizons\"\n",
    "\n",
    "print(f\"\\n   Positive IC horizons: {positive_horizons}\")\n",
    "print(f\"   Verdict: {verdict}\")\n",
    "print(f\"   {interpretation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53eb6d5c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Meta-Analysis: Failure Mode Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832ae30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# META-ANALYSIS: COMPREHENSIVE FAILURE SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìã META-ANALYSIS: FAILURE MODE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Collect all verdicts (these would be set by running the cells above)\n",
    "# For now, we'll compute them fresh\n",
    "\n",
    "summary = []\n",
    "\n",
    "# Breaker 1: Regime\n",
    "ic_range = regime_df['Mean_IC'].max() - regime_df['Mean_IC'].min() if 'regime_df' in dir() else 0\n",
    "sign_flip_b1 = (regime_df['Mean_IC'] < 0).any() if 'regime_df' in dir() else False\n",
    "if sign_flip_b1:\n",
    "    b1_verdict, b1_note = \"FAIL\", \"IC flips sign across regimes\"\n",
    "elif ic_range > 0.02:\n",
    "    b1_verdict, b1_note = \"WEAK\", f\"IC varies by {ic_range:.4f}\"\n",
    "else:\n",
    "    b1_verdict, b1_note = \"PASS\", \"Stable across regimes\"\n",
    "summary.append({'Breaker': '1. Time-Based Stress', 'Verdict': b1_verdict, 'Note': b1_note})\n",
    "\n",
    "# Breaker 2: Ablation\n",
    "if 'ablation_df' in dir():\n",
    "    ablation_no_baseline = ablation_df[ablation_df['Family'] != 'None (Baseline)']\n",
    "    most_critical = ablation_no_baseline.loc[ablation_no_baseline['IC'].idxmin()]\n",
    "    if most_critical['IC'] <= 0:\n",
    "        b2_verdict, b2_note = \"FAIL\", f\"{most_critical['Family']} is critical\"\n",
    "    elif most_critical['Pct_Retained'] < 50:\n",
    "        b2_verdict, b2_note = \"WEAK\", f\"Heavy reliance on {most_critical['Family']}\"\n",
    "    else:\n",
    "        b2_verdict, b2_note = \"PASS\", \"Diverse signal sources\"\n",
    "else:\n",
    "    b2_verdict, b2_note = \"N/A\", \"Not computed\"\n",
    "summary.append({'Breaker': '2. Feature Ablation', 'Verdict': b2_verdict, 'Note': b2_note})\n",
    "\n",
    "# Breaker 3: Permutation\n",
    "if 'perm_df' in dir():\n",
    "    if perm_df['Significant'].all():\n",
    "        b3_verdict, b3_note = \"PASS\", \"All tests significant\"\n",
    "    elif perm_df['Significant'].any():\n",
    "        b3_verdict, b3_note = \"WEAK\", \"Some tests not significant\"\n",
    "    else:\n",
    "        b3_verdict, b3_note = \"FAIL\", \"Indistinguishable from luck\"\n",
    "else:\n",
    "    b3_verdict, b3_note = \"N/A\", \"Not computed\"\n",
    "summary.append({'Breaker': '3. Permutation Tests', 'Verdict': b3_verdict, 'Note': b3_note})\n",
    "\n",
    "# Breaker 4: Noise\n",
    "if 'noise_df' in dir():\n",
    "    ic_at_5pct = noise_df[noise_df['Noise_Level'] == 0.05]['IC'].values[0]\n",
    "    ic_retained = noise_df[noise_df['Noise_Level'] == 0.05]['IC_Retained_Pct'].values[0]\n",
    "    if ic_at_5pct <= 0:\n",
    "        b4_verdict, b4_note = \"FAIL\", \"Collapses under noise\"\n",
    "    elif ic_retained < 50:\n",
    "        b4_verdict, b4_note = \"WEAK\", f\"{ic_retained:.0f}% retained at 5% noise\"\n",
    "    else:\n",
    "        b4_verdict, b4_note = \"PASS\", f\"{ic_retained:.0f}% retained at 5% noise\"\n",
    "else:\n",
    "    b4_verdict, b4_note = \"N/A\", \"Not computed\"\n",
    "summary.append({'Breaker': '4. Noise Injection', 'Verdict': b4_verdict, 'Note': b4_note})\n",
    "\n",
    "# Breaker 5: Horizon\n",
    "if 'horizon_df' in dir():\n",
    "    positive_horizons = horizon_df[horizon_df['IC'] > 0]['Horizon'].tolist()\n",
    "    if len(positive_horizons) == 1:\n",
    "        b5_verdict, b5_note = \"FAIL\", f\"Only at {positive_horizons[0]}d\"\n",
    "    elif len(positive_horizons) < 3:\n",
    "        b5_verdict, b5_note = \"WEAK\", f\"Only at {positive_horizons}\"\n",
    "    else:\n",
    "        b5_verdict, b5_note = \"PASS\", f\"Generalizes to {positive_horizons}\"\n",
    "else:\n",
    "    b5_verdict, b5_note = \"N/A\", \"Not computed\"\n",
    "summary.append({'Breaker': '5. Horizon Mismatch', 'Verdict': b5_verdict, 'Note': b5_note})\n",
    "\n",
    "summary_df = pd.DataFrame(summary)\n",
    "print(\"\\n\" + summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006a8712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FINAL VERDICT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üèÅ FINAL VERDICT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "n_pass = sum(1 for s in summary if s['Verdict'] == 'PASS')\n",
    "n_weak = sum(1 for s in summary if s['Verdict'] == 'WEAK')\n",
    "n_fail = sum(1 for s in summary if s['Verdict'] == 'FAIL')\n",
    "\n",
    "print(f\"\\n   ‚úÖ PASS: {n_pass}\")\n",
    "print(f\"   ‚ö†Ô∏è WEAK: {n_weak}\")\n",
    "print(f\"   ‚ùå FAIL: {n_fail}\")\n",
    "\n",
    "if n_fail >= 2:\n",
    "    overall = \"‚ùå MODEL NOT PROMOTABLE\"\n",
    "    recommendation = \"Multiple critical failures detected. Do not deploy.\"\n",
    "elif n_fail == 1 or n_weak >= 3:\n",
    "    overall = \"‚ö†Ô∏è MODEL REQUIRES FURTHER INVESTIGATION\"\n",
    "    recommendation = \"Address identified weaknesses before deployment.\"\n",
    "else:\n",
    "    overall = \"‚úÖ MODEL PASSES STRESS TESTS\"\n",
    "    recommendation = \"Model appears robust. Proceed with caution to live testing.\"\n",
    "\n",
    "print(f\"\\n   {overall}\")\n",
    "print(f\"   Recommendation: {recommendation}\")\n",
    "\n",
    "# Specific findings\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"üìù SPECIFIC FINDINGS:\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "failures = [s for s in summary if s['Verdict'] == 'FAIL']\n",
    "weaknesses = [s for s in summary if s['Verdict'] == 'WEAK']\n",
    "\n",
    "if failures:\n",
    "    print(\"\\n   FAILURES:\")\n",
    "    for f in failures:\n",
    "        print(f\"   - {f['Breaker']}: {f['Note']}\")\n",
    "\n",
    "if weaknesses:\n",
    "    print(\"\\n   WEAKNESSES:\")\n",
    "    for w in weaknesses:\n",
    "        print(f\"   - {w['Breaker']}: {w['Note']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"These diagnostics evaluate whether the model's apparent performance\")\n",
    "print(\"arises from structural signal or statistical coincidence.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0096371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAVE RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüíæ Saving stress test results...\")\n",
    "\n",
    "# Save summary\n",
    "summary_df.to_csv(OUTPUT_DIR / 'stress_test_summary.csv', index=False)\n",
    "print(f\"   ‚úÖ Saved: {OUTPUT_DIR / 'stress_test_summary.csv'}\")\n",
    "\n",
    "# Save detailed results\n",
    "results = {\n",
    "    'baseline_ic': baseline_ic,\n",
    "    'regime_analysis': regime_df.to_dict() if 'regime_df' in dir() else None,\n",
    "    'ablation_analysis': ablation_df.to_dict() if 'ablation_df' in dir() else None,\n",
    "    'permutation_tests': perm_df.to_dict() if 'perm_df' in dir() else None,\n",
    "    'noise_injection': noise_df.to_dict() if 'noise_df' in dir() else None,\n",
    "    'horizon_analysis': horizon_df.to_dict() if 'horizon_df' in dir() else None,\n",
    "    'summary': summary\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(OUTPUT_DIR / 'stress_test_detailed.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2, default=str)\n",
    "print(f\"   ‚úÖ Saved: {OUTPUT_DIR / 'stress_test_detailed.json'}\")\n",
    "\n",
    "print(\"\\n‚úÖ Stress test complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56af28a0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook systematically tested the model against 5 failure modes:\n",
    "\n",
    "| Breaker | Question | What It Reveals |\n",
    "|---------|----------|----------------|\n",
    "| 1. Time-Based Stress | Regime dependent? | Bull/bear, high/low vol sensitivity |\n",
    "| 2. Feature Ablation | Narrow dependency? | Which features the model relies on |\n",
    "| 3. Permutation Tests | Real or luck? | Statistical significance of signal |\n",
    "| 4. Noise Injection | Robust? | Overfitting to exact feature values |\n",
    "| 5. Horizon Mismatch | Generalizable? | Whether signal exists at other horizons |\n",
    "\n",
    "**Key Insight**: A model that passes all tests is more likely to survive out-of-sample. A model that fails should NOT be deployed regardless of in-sample performance."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
